---
title: "l1svr: An R Package for Support Vector Regression under l1 Regularization"
author: "Yuehao Bai, Hung Ho, Guillaume Pouliot, and Joshua Shea"
bibliography: l1svr.bib
output:
  github_document:
    toc: true
    keep_html: true
  html_document:
    toc: true
    keep_md: true
vignette: >
  %\VignetteIndexEntry{l1svr}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache = TRUE
)
```

## Introduction

The **l1svr** package allows the user to estimate and perform statistical inference for support vector regressions under l1 regularization (l1-SVR).
This vignette documents package installation and requirements, and demonstrates how the package may be used.
We refer the reader to @bai2021inference for technical details on how the estimation and inference are carried out.

## Installation and Requirements

<!-- **l1svr** can be installed from CRAN via -->
<!-- ```{r eval = FALSE} -->
<!-- install.packages("l1svr") -->
<!-- ``` -->

If you have the **devtools** package, you can install the latest version of the module directly from our GitHub repository via
```{r eval = FALSE}
devtools::install_github("jkcshea/l1svr")
```

A package for solving linear programs is required.
The user can choose from three options.

1. Gurobi and the Gurobi R package **gurobi**, which can be obtained from [Gurobi Optimization](http://www.gurobi.com/index).
   This option requires a Gurobi software license, which Gurobi Optimization offers at no cost to academic researchers.
   
2. CPLEX and the package **cplexAPI**,  which is available on CRAN.
   CPLEX can be obtained from [IBM](https://www.ibm.com/analytics/cplex-optimizer).
   This option requires a software license, which IBM offers at no cost to academic researchers.
   
3. The **lpSolveAPI** package, which is free and open-source, and available from CRAN.
   Note that **lpSolveAPI** is a wrapper for [lp_solve](http://lpsolve.sourceforge.net/5.5/), which is no longer actively developed.

We have provided the option to use **lpSolveAPI** because it appears to be the only interface for solving linear programs that can be installed solely through `install.packages`.
However, we _strongly_ recommend using Gurobi or CPLEX, since these are actively developed, much more stable, and typically an order of magnitude faster than **lpSolveAPI**.
A very clear installation guide for Gurobi can be found [here](https://cran.r-project.org/package=prioritizr/vignettes/gurobi_installation.html)


<!--
    i. The **lpSolveAPI** package, which is free and open-source, and available from CRAN.
    Note that **lpSolveAPI** is a wrapper for [lp_solve](http://lpsolve.sourceforge.net/5.5/), which is no longer
    actively developed.
-->

<!-- ## Background -->

<!-- Support vector regression with l1-norm regularization extends the support vector machine classification method to the regression problem, and is designed to reproduce the good out-of-sample performance of SVM classification in the regression setting. -->

<!-- In support vector regressions, the parameters of a linear regression model are estimated by minimizing a loss function that linearly penalizes the errors exceeding a threshold `epsilon`. -->
<!-- The l1-norm regularization includes an additional penalty equal to the l1-norm of the coefficient estimates. -->
<!-- The extent of the regularization is controlled by the tuning parameter `lambda`, which scales the penalty on the coefficient estimates. -->

<!-- As depicted below, the loss functions for median regression and support vector regression are similar, which suggests a close analogy between the method for inference. -->

<!-- ```{r, echo = FALSE, out.width = '50%', fig.show = 'hold', fig.align = 'default'} -->
<!-- knitr::include_graphics(c("images/qr-loss.png","images/svr-loss.png")) -->
<!-- ``` -->


## Usage Demonstration

```{r, load-package, include = FALSE}
source('svr-functions.R')
```

```{r, gen-data, include = FALSE}
library(MASS)
set.seed(10L)
n <- 500
mu <- c(1, 2)
Sigma <- matrix(c(1, -2, 3, 2), nrow = 2)
Sigma <- Sigma %*% t(Sigma)
sdMat <- diag(1/sqrt(diag(Sigma)))
corrMat <- sdMat %*% Sigma %*% sdMat
beta <- c(-2, 1, 0.5)
X <- as.matrix(mvrnorm(n = n, mu = mu, Sigma = Sigma))
x <- cbind(1, X)
u <- rnorm(n, 0, 12)
y <- x[, 1:3] %*% beta + u
simdata <- data.frame(cbind(y, x[, -1]))
colnames(simdata) <- c('y', 'x1', 'x2')
```

The package includes the data set `simdata`, which we will use to demonstrate the features of the package.
<!-- The data generating process is `y ~ -2 + x1 + 0.5 * x2 + u`, where `u` is an error term unobserved to the researcher. -->

```{r, show-data}
knitr::kable(head(simdata))
```


### Basic estimation and inference

In support vector regressions, the parameters of a linear regression model are estimated by minimizing a loss function that linearly penalizes the errors exceeding a threshold `epsilon`, as depicted in the figure below.

```{r, echo = FALSE, out.width = '50%', fig.show = 'hold', fig.align = 'center'}
knitr::include_graphics("images/svr-loss.png")
```

The l1-norm regularization includes an additional penalty proportional to the l1-norm of the coefficient estimates.
The extent of the regularization is controlled by the tuning parameter `lambda`, which scales the l1-penalty on the coefficient estimates.

To estimate a linear regression model using the **l1svr** package, the user must pass a data set, a regression equation, a bandwidth `epsilon`, and a tuning parameter `lambda` to the function `l1svr`.
The user may also choose which solver to use by setting the `solver` argument to `'gurobi'`, `'cplexapi'`, or `'lpsolveapi'`. 
By default, `solver = 'gurobi'`.

```{r, basic-demo}
fit1 <- l1svr(formula = y ~ 1 + x1 + x2,
              data = simdata,
              epsilon = 7,
              lambda = 20,
              solver = 'gurobi')
summary(fit1)
```

The coefficient estimates are stored in the entry `$coefficients` of the output, and the p-values are stored in the entry `$pvalues`.
By default, statistical inference is performed assuming that the errors are homoskedastic.

Inference under heteroskedasticity is also possible, but requires the density of the errors to be estimated.
To enable this feature, the user must provide the additional tuning parameters needed for density estimation.
The **l1svr** package estimates the density of the errors using the procedure described in @powell1991estimation, which involves two positive  scalar tuning parameters, `h` and `kappa`.
They determine the bandwidth used for density estimation, and the bandwidth is increasing in both parameters. 
The parameter `h` determines the quantiles of a standard normal distribution used to define the bandwidth, and the parameter `kappa` directly scales the bandwidth.
Inference under heteroskedasticity will not be performed unless both `h` and `kappa` are passed.
We refer the reader to the Online Appendix of @bai2021inference for additional details on how inference is performed under heteroskedasticity.

```{r, hetero-demo}
fit2 <- l1svr(formula = y ~ 1 + x1 + x2,
              data = simdata,
              epsilon = 7,
              lambda = 20,
              h = 2,
              kappa = 1.75)
summary(fit2)
```

If desired, the user may turn off the inference procedure by setting `inference = FALSE`.

### Constructing confidence intervals

The `l1svr` function is able to construct confidence intervals by inverting the test procedure.
This entails the function implementing the l1-SVR regression rankscore test under a sequence of null hypotheses for each coefficient estimate.
The end points of the confidence interval correspond to the null hypotheses under which the p-value of the test statistic is equal to the size of the test, up to some tolerance.
This is a computationally intensive procedure and is disabled by default.

To enable the estimation of confidence intervals, set the argument `confidence.level` to a value between 0 and 1.
As its name indicates, this argument also determines the confidence level of the intervals.
The iterative procedure will terminate when the difference between the p-value of the endpoints of the confidence interval and the size of the test (`1 - confidence.level`) is within the level of tolerance.
By default, the tolerance is equal to `1e-3`, but can be adjusted using the argument `confidence.tol`.
The maximum number of iterations performed may also be set using `confidence.iter`.
In the example below, the confidence intervals are constructed assuming that the errors are homoskedastic.

```{r, ci-demo}
fit3 <- l1svr(formula = y ~ 1 + x1 + x2,
              data = simdata,
              epsilon = 7,
              lambda = 20,
              confidence.level = 0.95,
              confidence.iter = 20,
              confidence.tol = 2e-3)
summary(fit3)
```


In the example above, the user is informed that the estimation of some of the confidence intervals has been terminated before the p-value at the endpoints are within tolerance to the size of the test since the iteration limit was reached.
The estimated confidence interval may therefore not reflect the confidence level set by the user.
In addition to warning the user of these instances, the function returns the p-values at the end points of each confidence interval in the entry `$ci` of the output, along with details on the iterative procedure.

```{r, full-ci-output}
fit3$ci
```

Another reason that the estimation of confidence intervals may terminate prematurely is that the estimated endpoints across successive iterations have become sufficiently small.
This implies that the solver is unable to perform the test at the level of precision required for the p-value of the bound to fall within tolerance to the size of the test.
The user can try to avoid this by reducing `confidence.same`, which sets the tolerance level determining whether two bounds from successive iterations of the procedure are sufficiently close.
By default, this parameter is equal to `1e-06`.
However, since the limitation pertains to the solver, there is no guarantee that reducing this parameter will improve the estimate of the confidence intervals.

<!-- ## Additional output -->

<!-- In addition to returning the coefficient estimates and p-values, the `l1svr` function returns other quantities that may be of interest to the usuer. -->

<!-- 1. The positive and negative residuals of the l1-SVR problem are returned in the entries `$primalPositive` and `$primalNegative` of the output, respectively. -->

<!-- 1. The solutions to the dual problem are returned in the entries `$dualPositive` and `$dualNegative`. -->
<!-- These two vectors are used to conduct inference using a rankscore test. -->

<!-- 1. The number of observations used in estimation is returned in the entry `$N` of the output. -->

<!-- The lengths of `$primalPositive`, `$primalNegative`, `$dualPositive`, and `$dualNegative` are all equal to `$N`. -->
<!-- Each entry of each vector corresponds to an observation used for estimation. -->
<!-- Entries of `$primalPositive` are equal to the residual for observations with strictly positive residuals, and are equal to `0` otherwise. -->
<!-- Likewise, entries of `$primalNegative` are equal to the negative of the residual (i.e. a positive number) for observations with strictly negative residuals, and are equal to `0` otherwise. -->

<!-- The entries in the vector `$dualPositive` are equal to `-1` for observations with residuals greater than the SVR threshold `abs(epsilon)`. -->
<!-- The entries are equal to `0` for observations with residuals less than `abs(epsilon)`. -->
<!-- The entries will be non-zero for observations with residuals equal to `abs(epsilon)`. -->

<!-- The entries in the vector `$dualNegative` adhere to a similar pattern. -->
<!-- That is, the entries corresponding to observations with residuals less than `-abs(epsilon)` are equal to `-1`; the entries corresponding to observations with negative residuals greater than `-abs(epsilon)` are equal to `0`; and the entries with residuals equal to `-abs(epsilon)` are non-zero. -->

<!-- The difference between `$dualNegative` and `$dualPositive` is thus a monotonic transformation of the residuals from the l1-SVR regression, as shown in the figure below. -->

<!-- ```{r, alpha-demo, fig.align='center'} -->
<!-- # Plot the difference between the dual variables -->
<!-- residuals <- simdata$y - as.matrix(cbind(1, simdata[, -1])) %*% fit3$coef -->
<!-- alphaDiffs <- fit3$dualNegative - fit3$dualPositive -->
<!-- plot(x = residuals, y = alphaDiffs,  -->
<!--      xlab = 'Residuals', ylab = 'Difference') -->
<!-- ``` -->

<!-- If the difference is negative, then the corresponding observation lies outside the SVR bandwidth interval `[-epsilon, epsilon]` and has a negative residual. -->
<!-- If the difference is positive, then the corresponding observation lies outside the SVR bandwidth but has a positive residual. -->
<!-- If the difference is 0, then the corresponding observation is in the interior of the bandwidth. -->
<!-- Finally, in all other cases, the observation is on the boundary of the bandwidth and define the support vectors. -->
<!-- This is illustrated in the figure below. -->

<!-- ```{r, echo = FALSE, fig.show = 'hold', fig.align = 'center'} -->
<!-- knitr::include_graphics("images/cs.png") -->
<!-- ``` -->

## Help, Feature Requests and Bug Reports

Please post an issue on the [GitHub repository](https://github.com/jkcshea/l1svr/issues).

## References
